---
layout: post
title: How Do We Attack a Contrastive Model?
tags: [contrastive, attack, CLIP]
authors: Paynter, Jonathan, MIT ORC
---

> Learn about contrastive learners and one recent take on how to attack them. These attacks have broader implications for attacks on foundation models that propagate to many downstream applications.

## Goal

![overview](Goal_fig.png)
*The basic setup for a contrastive model used in a downstream task, and the goal of the attacker. (figure by author)*

This post provides an overview of [*Poisoning and Backdooring Contrastive Learning*](https://arxiv.org/abs/2106.09667) by Carlini and Terzis. I hope that after reading the post you know a bit about contrastive learning, why attacking a contrastive model is different than attacking a supervised machine learning model, and the key aspects of this paper.

## TL;DR

* A contrastive learning model determines a **representation of unlabeled input data** where elements that are similar are close together, and dissimilar elements are **contrasted**, and further apart. 
* Because a contrastive model determines a representation of the data, and does not classify, an **attack on a contrastive model** can't leverage an incorrect classification label. It needs to ensure that a certain input is represented incorrectly - that it is **embedded in the vector space in the 'wrong' area**.
* The primary result for the authors is that by **poisoning just 0.0001%** of the input data or applying a backdoor trigger patch to 0.005% of the data, their malicious attacks succeed.

## What is contrastive learning?

> A contrastive learner outputs a new representation of the data for use in downstream tasks (classification, zero-shot learner, etc).

A contrastive model learns a representation of the input data, which can then be leveraged in downstream tasks to better effect than the original data representation. There are many possible downstream tasks including classification and zero-shot learning. A contrastive model is multi-modal. It takes paired input data - such as images with text captions - and learns a lower dimensional embedding for each, where the embeddings of the paired inputs are similar. At initial glance, there are aspects of this might seem like supervised learning - is the caption playing the role of the label for the image? 

Not really; there are actually important differences between a supervised machine learning model that classifies images into categories and a contrastive model that takes images and captions as inputs. Because there is no parsing of the text caption to select a single label, the model leverages all of the caption's text. For example, the below figure shows the goal - capture a representation of the images (green) and a representation of the text (purple) so that the embedding maximizes similarity (blue). But we don't need to parse the text caption to just "pup" or "dog" - we retain the full text.

![CLIP-mod](CLIP_goal.png)
*The goal of a contrastive learner is lower-dimensional representations of text and images such that input pairs have maximal similarity. (cropped figure from Open AI's CLIP)*

**Wait! If the text isn't parsed to a known set of labels, how do we determine the objective loss?**

Cleverly! The key feature in training a contrastive model is to present pairs of similar inputs and pairs of dissimilar inputs. The objective function then seeks to align similar pairs together in the embedded space, and contrast dissimilar pairs further apart (a pull and a push). We have known similar pairs from the input data, and we can shuffle the text from the input pairs to create dissimilar pairs. We can also enhance the training set by modifying aspects of the known similar pairs from the training data to create additional similar pairs - manipulating the color scaling, cropping, rotating, etc. 

![training](Training.png)
*The basic training idea for a contrastive model - maximizing the similarity of known pairs and minimizing the similarity of mismatched pairs. (figure by author leveraging Open AI's CLIP example inputs)*

**Let's see a specific example with CLIP, and how it is leveraged for downstream tasks.**

Carlini and Terzis use CLIP from Open AI ([paper](https://arxiv.org/abs/2103.00020); [blog](https://openai.com/blog/clip/)) as the contrastive model to attack. CLIP was developed to "learn visual concepts from natural language supervision". You can try out an implementation for yourself at [Colab](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb). For a nice summary of using CLIP for zero-shot learning, see this [blog](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab).  


![CLIP-fig2](CLIP_fig2.PNG)
*This is Figure 1 from Open AI's CLIP paper (blog version). For zero-shot learning: We see that in step (1) we input image / text pairs, where each is encoded (purple and green) to maximize the encoding similarities (blue). Then, in step (2) we create a list of possible data inputs of the form "A photo of a {object}" for many different objects. In step (3), we input a photo and select the caption with the closest representation.*

Additionally, because there is no deliberate label selection, the model isn't constrained to the selected set of labels (i.e. 1000 labels in ImageNet). This then allows for more input data since any paired data, such as image with caption, can go into the training set wih no label crafting (...but this can also provide an opening for an adversary!). CLIP uses a training set of 400 million image / text pairs.

## A brief description of poisoning and backdoor attacks

>The key idea in attacking a supervised machine learning model is to introduce malicious training data that has an adversarially-desired label.

Two options a malicious actor might use to degrade a machine learning model are data poisoning and backdoor attacks. Both leverage the introduction of malicious examples into the machine learning model's training data to shape the output in a certain way. 

A **poisoning** attack is when an adversary introduces malicious training examples to a machine learning classifier so that it outputs an adversarially-desired label for a certain input. For example, we introduce images of a certain black cat with the label "dog" so that in practice black cats are misclassified in the future. 

A **backdoor** attack is when an adversary introduces a patch on some training examples so that the machine learning classifier will output an adversarially-desired label anytime the trigger-patch is detected. It creates a *backdoor* to the model so that anytime we want to foce a certain output, we insert the trigger patch on the image. For a nice overview of malicious actions, see this [blog](https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db). 

## What makes an attack on a contrastive learning model different?

> There might be multiple downstream tasks for a contrastive learner, and so the focus is on forcing the model to "misrepresent" some input data with an otherwise incorrect embedding. 

A contrastive learner is unsupervised - so if we want to attack it, there is no ability to provide an "adversarially-desired label" with a malicious training example. We can't flip labels, or insert a backdoor on images that have our label paired with them. This means we can't apply a traditional attack method. In fact, because the output of a contrastive learner is a representation that might be used in many different downstream tasks, it's not immediately obvious what an attack means! Are we attempting to stop the contrastive learner from providing a good representation for zero-shot learning? incorrectly classify in a downstream classification task? something else?

Generally, we might want the contrastive learner to "misrepresent" certain input data; we want to force some inputs to the "wrong area" of the embedding space. If we do this, then hopefully, our malicious attack propagates into the downstream task.

## Poisoning and Backdooring Contrastive Learning

> The primary result for the authors is that by **poisoning just 0.001%** of the input data or applying a backdoor trigger patch to 0.005% of the data, their malicious attacks succeed.

![gif](GIF_poison_v2.gif)
*The poisoning attack on a contrastive learner. A certain number of malicious examples are introduced - here images of the lovely poodle-mix along with captions that include the adversarially intended label: plane. (figure by author using cropped parts of OpenAI's CLIP figure 1)*

Carlini and Terzis address both poisoning and backdoor attacks on a multi-modal contrastive learner that uses images and captions. They introduce different numbers of malicious samples in different iterations to explore how much of the training data needs to be poisoned or backdoored for a successful attack. 
 
 
 **Poisoning experiements**
 
 For both downstream classification and zero-shot learning, the eventual output is a label - in this case, a label for an image. For a poisoning attack, the authors select an intended malicious image and a target label. They then generate many different captions for the malicious image that all include the adversarially-desired label. From this, they construct malicious training sets of various sizes. 
 
 The authors measure attack success in two ways. First, the average rank of the output label, where the adversarially-desired label has the highest value. Second, with a binary check on whether the adversarially-desired label is in the top-5 outputs. 
 
 > A few poisoned data pairs go a long way - even with only two poisoned images in the 3 million examples used (from the conceptual captions dataset), the model misclassifies the targeted image 60% of the time with zero-shot learning. 
 
  **Backdoor attack experiements**

 
 Backdoor z-score....
 
 > Backdoor ... 
 
 ## An implication for foundation models
 
CLIP is part of a growing set of [foundation models](https://arxiv.org/pdf/2108.07258.pdf) that set the scene for widespread adoption of transfer learning, where many task-specific models are built on top of these large, compute-intensive, adaptable models. One of the aspects that makes these foundation models possible is the collection of extremely large amounts of training data, which is possible when we don't need curated labels, and the builders of the large models can scrape widely for input data. This presents an opportunity for malicious actors to design attacks based on scattering malicious examples across the web so that the attacked-representation becomes an input to many future models. 
 
 As models start to commonly adopt a transfer learning approach, the goal of attacking a foundation model grows for malicious actors. Carlini and Terzis present one method for attacking a contrastive learner that demonstrates the utility of these attacks, and the low volume of malicious examples needed.
 
## References 

* Bommasani et al. [*On the Opportunities and Risks of Foundation Models*](https://arxiv.org/pdf/2108.07258.pdf). ArXiv, 2021. 
* Carlini and Terzis. [*Poisoning and Backdooring Contrastive Learning*](https://arxiv.org/abs/2106.09667). ArXiv, 2021.
* Ilmoi. [*Poisoning attacks on Machine Learning*](https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db). Towards Data Science, 2021. 
* Radfor et al. [*Learning Transferable Visual Models From Natural Language Supervision*](https://arxiv.org/abs/2103.00020). ArXiv, 2021. Also: [blog](https://openai.com/blog/clip/)) and [Colab](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb).
* Tiu. [*Understanding Zero-Shot Learning - Making ML More Human*](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab). Towards Data Science, 2021. 
