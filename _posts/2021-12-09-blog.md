---
layout: post
title: How Do We Attack a Contrastive Model?
tags: [contrastive, attack, CLIP]
authors: Paynter, Jonathan, MIT ORC
---

> Let's learn about what it takes to attack a contrastive learners

### Goal

This post provides an overview of [*Poisoning and Backdooring Contrastive Learning*](https://arxiv.org/abs/2106.09667) by Carlini and Terzis. I hope that after reading the post you know a bit about contrastive learning, why attacking a constrastive model is different than attacking a supervised machine learning model, and the key aspects of this paper.

### TL;DR

* A contrastive learning model determines a **representation of unlabeled input data** where elements that are similar are close together, and dissimilar elements elements are **contrasted**, and further apart. 
* Because a contrastive model determines a representation of the data, and does not classify, an **attack on a contrastive model** can't leverage an incorrect classification label. It needs to ensure that a certain input is represented incorrectly - that it is **embedded in the vector space in the 'wrong' area**.
* The primary result for the authors is that by **poisoning just 0.001%** of the input data or applying a backdoor trigger patch to 0.005% of the data, their malicious attacks succeed.

### What is contrastive learning?

Carlini and Terzis use CLIP from Open AI ([paper](https://arxiv.org/abs/2103.00020); [blog](https://openai.com/blog/clip/)) as the contrastive model to attack. CLIP was developed to "learn visual concepts from natural language supervision". You can try out an implementation for yourself at [Colab](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb).

A contrastive model learns a representation of the input data, and can then be leveraged in downstream tasks, such as classification or zero-shot learning. For a nice summary of using CLIP for zero-shot learning, see this [blog](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab).  

![CLIP-fig1](CLIP_fig1.PNG)
*This is Figure 1 from Open AI's CLIP paper. For zero-shot learning: We see that in step (1) we input a pair of text and image, where each is encoded (purple and green) and we are working to minimize the encoding differences (blue). Then, in step (2) we create a list of possible data inputs of the form "A photo of a {object}" for many different objects. In step (3), we input a photo and select the caption with the cosest representation.*

One of the nice elements of this model is that we can leverage raw image and caption pairs - we don't select a single label for an image or attempt to parse the text. With enough data, this allows for a richer representation of the images, and it allows for a much larger set of input data as we can leverage any captioned pictures found on the internet (...but this can also provide an opening for an adversary!). 

> A contrastive learner outputs a new representation of the data for downstream tasks (classification, zero-shot learner, etc).

### A brief description of poisoning and backdooring

Two options a malicious actor might use to degrade a machine learning model are data poisoning and backdoor attacks. Both leverage the introduction of malicious examples into the machine learning model's training data to shape the output in a certain way. A **poisoning** attack is when an adversary introduces malicious training examples to a machine learning classifier so that it outputs an adversarially-desired label for a certain input. A **backdoor** attack is when an adversary introduces a patch on some training examples so that the machine learning classfier will output an adversarially-desired label anytime the trigger-patch is detected. For a nice overview of malicious actions, see this [blog](https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db). 

>The key idea in attacking a supervised machine learning model is to introduce malicious training data that has an adversarially-desired label.

### What makes an attack on a contrastive learning model different?

A contrastive learner is unsupervised - so if we want to attack it, there is no ability to provide an "adversarially-desired label" with a malicious training example. So we can't apply a traditional attack method. In fact, because the output of a contrastive learner is a representation that might be used in many different downstream tasks, it's not immediately obvious what an attack means! Are we attempting to stop the contrastive learner from providing a good representation for zero-shot learning? downstream classification? something else?

>There might be multiple goals for attacking a constrastive learner related to a variety of downstream tasks. 

### Poisoning and Backdooring Contrastive Learning

 Carlini and Terzis address both poisoning and backdoor attacks on a multi-modal contrastive learner that uses images and captions. They introduce different numbers of malicious samples in different iterations to explore how much of the training data needs to be poisoned or backdoored for a successful attack. 
 
 The authors define success....
 
 Numerical experiements....
 
 Backdoor z-score....
 
 ### Conclusion
 
### References 

*Carlini and Terzis. [*Poisoning and Backdooring Contrastive Learning*](https://arxiv.org/abs/2106.09667). ArXiv, 2021.
*Radfor et al. [*Learning Transferable Visual Models From Natural Language Supervision*](https://arxiv.org/abs/2103.00020). ArXiv, 2021. Also: [blog](https://openai.com/blog/clip/)) and [Colab](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb).
*Tiu. [*Understanding Zero-Shot Learning - Making ML More Human*](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab). Towards Data Science, 2021. 
*Ilmoi. [*Poisoning attacks on Machine Learning*](https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db). Towards Data Science, 2021. 
