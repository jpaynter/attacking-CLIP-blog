---
layout: post
title: How Do We Attack a Contrastive Model?
tags: [contrastive, attack, CLIP]
authors: Paynter, Jonathan, MIT ORC
---

> Learn about contrastive learners and one take on how to attack them

### Goal

This post provides an overview of [*Poisoning and Backdooring Contrastive Learning*](https://arxiv.org/abs/2106.09667) by Carlini and Terzis. I hope that after reading the post you know a bit about contrastive learning, why attacking a constrastive model is different than attacking a supervised machine learning model, and the key aspects of this paper.

### TL;DR

* A contrastive learning model determines a **representation of unlabeled input data** where elements that are similar are close together, and dissimilar elements elements are **contrasted**, and further apart. 
* Because a contrastive model determines a representation of the data, and does not classify, an **attack on a contrastive model** can't leverage an incorrect classification label. It needs to ensure that a certain input is represented incorrectly - that it is **embedded in the vector space in the 'wrong' area**.
* The primary result for the authors is that by **poisoning just 0.001%** of the input data or applying a backdoor trigger patch to 0.005% of the data, their malicious attacks succeed.

### What is contrastive learning?

> A contrastive learner outputs a new representation of the data for downstream tasks (classification, zero-shot learner, etc).

A contrastive model learns a representation of the input data, which can then be leveraged in downstream tasks, such as classification or zero-shot learning. It takes paired input data - such as images with text captions - and learns to embed similar images and text in the same portion of the embedded vector space. At initial glance, this might seem somewhat like a supervised model - is the caption playing the role of the label? But there are actually very important differences. Because there is no parsing of the text caption to select a single label, the model leverages all of the caption by embedding the text. But if the text isn't parsed to a known set of labels, what is the goal of the model? All it sees are known pairs?

Not quite. The key feature in training a contrastive model is to present pairs of similar inputs and pairs of disimilar inputs. The objective function then seeks to align similar pairs together in the embedded space, and disimilar pairs further apart. We can leverage data transformations from the input data to enhance our training set. We can modify aspects of the known similar pairs from the training data to create additional similar pairs - manipulating the color scaling, cropping, rotating, etc. And, we can generate the disimliar pairs by shuffling the images and text.

Additionally, because there is no deliberate label selection, the model isn't contrained to the selected set of labels (i.e. 1000 labels in ImageNet). This then allows for more possible input data as any paired data, such as image with caption, can go into the training set (...but this can also provide an opening for an adversary!). 

Carlini and Terzis use CLIP from Open AI ([paper](https://arxiv.org/abs/2103.00020); [blog](https://openai.com/blog/clip/)) as the contrastive model to attack. CLIP was developed to "learn visual concepts from natural language supervision". You can try out an implementation for yourself at [Colab](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb). For a nice summary of using CLIP for zero-shot learning, see this [blog](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab).  

![CLIP-fig1](CLIP_fig1.PNG)
*This is Figure 1 from Open AI's CLIP paper. For zero-shot learning: We see that in step (1) we input a pair of text and image, where each is encoded (purple and green) and we are working to minimize the encoding differences (blue). Then, in step (2) we create a list of possible data inputs of the form "A photo of a {object}" for many different objects. In step (3), we input a photo and select the caption with the cosest representation.*

### A brief description of poisoning and backdooring

>The key idea in attacking a supervised machine learning model is to introduce malicious training data that has an adversarially-desired label.

Two options a malicious actor might use to degrade a machine learning model are data poisoning and backdoor attacks. Both leverage the introduction of malicious examples into the machine learning model's training data to shape the output in a certain way. A **poisoning** attack is when an adversary introduces malicious training examples to a machine learning classifier so that it outputs an adversarially-desired label for a certain input. A **backdoor** attack is when an adversary introduces a patch on some training examples so that the machine learning classfier will output an adversarially-desired label anytime the trigger-patch is detected. For a nice overview of malicious actions, see this [blog](https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db). 

### What makes an attack on a contrastive learning model different?

A contrastive learner is unsupervised - so if we want to attack it, there is no ability to provide an "adversarially-desired label" with a malicious training example. So we can't apply a traditional attack method. In fact, because the output of a contrastive learner is a representation that might be used in many different downstream tasks, it's not immediately obvious what an attack means! Are we attempting to stop the contrastive learner from providing a good representation for zero-shot learning? downstream classification? something else?

>There might be multiple goals for attacking a constrastive learner related to a variety of downstream tasks. 

### Poisoning and Backdooring Contrastive Learning

> Summary of paper...

 Carlini and Terzis address both poisoning and backdoor attacks on a multi-modal contrastive learner that uses images and captions. They introduce different numbers of malicious samples in different iterations to explore how much of the training data needs to be poisoned or backdoored for a successful attack. 
 
 The authors define success....
 
 Numerical experiements....
 
 Backdoor z-score....
 
 ### Conclusion
 
### References 

* Carlini and Terzis. [*Poisoning and Backdooring Contrastive Learning*](https://arxiv.org/abs/2106.09667). ArXiv, 2021.
* Radfor et al. [*Learning Transferable Visual Models From Natural Language Supervision*](https://arxiv.org/abs/2103.00020). ArXiv, 2021. Also: [blog](https://openai.com/blog/clip/)) and [Colab](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb).
* Tiu. [*Understanding Zero-Shot Learning - Making ML More Human*](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab). Towards Data Science, 2021. 
* Ilmoi. [*Poisoning attacks on Machine Learning*](https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db). Towards Data Science, 2021. 
