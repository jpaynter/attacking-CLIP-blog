---
layout: post
title: How Do We Attack a Contrastive Model?
tags: [contrastive, attack, CLIP]
authors: Paynter, Jonathan, MIT ORC
---

> Learn about contrastive learners and one recent take on how to attack them. These attacks have wider implications for transfer learning.

## Goal

![overview](Goal_fig.png)
*The basic setup for a contrastive model used in a downstream task.*

This post provides an overview of [*Poisoning and Backdooring Contrastive Learning*](https://arxiv.org/abs/2106.09667) by Carlini and Terzis. I hope that after reading the post you know a bit about contrastive learning, why attacking a constrastive model is different than attacking a supervised machine learning model, and the key aspects of this paper.

## TL;DR

* A contrastive learning model determines a **representation of unlabeled input data** where elements that are similar are close together, and dissimilar elements are **contrasted**, and further apart. 
* Because a contrastive model determines a representation of the data, and does not classify, an **attack on a contrastive model** can't leverage an incorrect classification label. It needs to ensure that a certain input is represented incorrectly - that it is **embedded in the vector space in the 'wrong' area**.
* The primary result for the authors is that by **poisoning just 0.001%** of the input data or applying a backdoor trigger patch to 0.005% of the data, their malicious attacks succeed.

## What is contrastive learning?

> A contrastive learner outputs a new representation of the data for downstream tasks (classification, zero-shot learner, etc).

A contrastive model learns a representation of the input data, which can then be leveraged in downstream tasks, such as classification or zero-shot learning. It takes paired input data - such as images with text captions - and learns to embed similar images and text in the same portion of the embedded vector space. At initial glance, this might seem somewhat like a supervised model - is the caption playing the role of the label? But there are actually very important differences. Because there is no parsing of the text caption to select a single label, the model leverages all of the caption by embedding the text. For example, the below figure shows the goal - capture a representation of the images (green) and a representation of the text (purple) so that the embedding maximizes similarity (blue).

![CLIP-mod](CLIP_goal.png)
*The goal of a contrastive learner (cropped figure from Open AI's CLIP).*

**Wait! If the text isn't parsed to a known set of labels, how do we determine the objective loss?**

Cleverly! The key feature in training a contrastive model is to present pairs of similar inputs and pairs of disimilar inputs. The objective function then seeks to align similar pairs together in the embedded space, and disimilar pairs further apart. We have known similar pairs from the input data, and we can shuffle the input pairs to create disimilar pairs. Additional methods include leveraging data transformations from the input data to enhance our training set by modifying aspects of the known similar pairs from the training data to create additional similar pairs - manipulating the color scaling, cropping, rotating, etc. 

![training](Training.png)
*The basiic training idea for a contrastive model - maximizing the similarity of known pairs and minimizing the similarity of mismatched pairs.*

**Let's see a specific example with CLIP, and how it is leveraged for downstream tasks.**

Carlini and Terzis use CLIP from Open AI ([paper](https://arxiv.org/abs/2103.00020); [blog](https://openai.com/blog/clip/)) as the contrastive model to attack. CLIP was developed to "learn visual concepts from natural language supervision". You can try out an implementation for yourself at [Colab](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb). For a nice summary of using CLIP for zero-shot learning, see this [blog](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab).  


![CLIP-fig2](CLIP_fig2.PNG)
*This is Figure 1 from Open AI's CLIP paper, from the blog. For zero-shot learning: We see that in step (1) we input a pair of text and image, where each is encoded (purple and green) and we are working to minimize the encoding differences (blue). Then, in step (2) we create a list of possible data inputs of the form "A photo of a {object}" for many different objects. In step (3), we input a photo and select the caption with the cosest representation.*

Additionally, because there is no deliberate label selection, the model isn't contrained to the selected set of labels (i.e. 1000 labels in ImageNet). This then allows for more possible input data as any paired data, such as image with caption, can go into the training set (...but this can also provide an opening for an adversary!). 

## A brief description of poisoning and backdoor attacks

>The key idea in attacking a supervised machine learning model is to introduce malicious training data that has an adversarially-desired label.

Two options a malicious actor might use to degrade a machine learning model are data poisoning and backdoor attacks. Both leverage the introduction of malicious examples into the machine learning model's training data to shape the output in a certain way. 

A **poisoning** attack is when an adversary introduces malicious training examples to a machine learning classifier so that it outputs an adversarially-desired label for a certain input. For example, we introduce images of a certain black cat with the label "dog" so that in practice black cats are misclassified. 

A **backdoor** attack is when an adversary introduces a patch on some training examples so that the machine learning classfier will output an adversarially-desired label anytime the trigger-patch is detected. We've created a *backdoor* to the model so that anytime we want to foce a certain output, we insert the trigger patch on the image. For a nice overview of malicious actions, see this [blog](https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db). 

## What makes an attack on a contrastive learning model different?

> There might be multiple goals for attacking a constrastive learner related to a variety of downstream tasks, and so the focus is on forcing the model to "mis-represent" some input data. 

A contrastive learner is unsupervised - so if we want to attack it, there is no ability to provide an "adversarially-desired label" with a malicious training example. We can't flip labels, or insert a backdoor on images that have our label paired with them. So, we can't apply a traditional attack method. In fact, because the output of a contrastive learner is a representation that might be used in many different downstream tasks, it's not immediately obvious what an attack means! Are we attempting to stop the contrastive learner from providing a good representation for zero-shot learning? incorrectly classify in a downstream classification task? something else?

Generally, we might want the contrastive learner to "mis-represent" certain input data; we want to force some inputs to the "wrong area" of the embedding space. If we do this, then we can propogate our malicious attack into the downstream task.

## Poisoning and Backdooring Contrastive Learning

> The primary result for the authors is that by **poisoning just 0.001%** of the input data or applying a backdoor trigger patch to 0.005% of the data, their malicious attacks succeed.

![gif](GIF_poison_v2.gif)
*The poisoning attack on a contrastive learner. A certain number of malicious examples are introduced - here images of the lovely poodle-mix along with captions that include the adversarially intended label (plane).*

Carlini and Terzis address both poisoning and backdoor attacks on a multi-modal contrastive learner that uses images and captions. They introduce different numbers of malicious samples in different iterations to explore how much of the training data needs to be poisoned or backdoored for a successful attack. 
 
 The authors define success....
 
 Numerical experiements....
 
 Backdoor z-score....
 
 ## An implication for foundation models
 
 CLIP is part of a growing set of [foundation models](https://arxiv.org/pdf/2108.07258.pdf) that set the scene for widespread adoption of transfer learning, where many task-specific models are built on top of these large, compute-intensive, adaptable models. One of the aspects that makes these foundation models possible is the collection of extremely large amounts of training data, which is possible when we don't need curated labels, and the builders of the large models can scrape widely for input data. This presents an opportunity for malicious actors to design attacks based on scattering malicious examples across the web so that the attacked-representation becomes an input to many future models. 
 
 As models start to commonly adopt a transfer learning approach, the goal of attacking a foundation model grows for malicious actors. Carlini and Terzis present one method for attacking a contrastive learner that demonstrates the utility of these attacks, and the low volume of malicious examples needed.
 
## References 

* Bommasani et al. [*On the Opportunities and Risks of Foundation Models*](https://arxiv.org/pdf/2108.07258.pdf). ArXiv, 2021. 
* Carlini and Terzis. [*Poisoning and Backdooring Contrastive Learning*](https://arxiv.org/abs/2106.09667). ArXiv, 2021.
* Ilmoi. [*Poisoning attacks on Machine Learning*](https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db). Towards Data Science, 2021. 
* Radfor et al. [*Learning Transferable Visual Models From Natural Language Supervision*](https://arxiv.org/abs/2103.00020). ArXiv, 2021. Also: [blog](https://openai.com/blog/clip/)) and [Colab](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb).
* Tiu. [*Understanding Zero-Shot Learning - Making ML More Human*](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab). Towards Data Science, 2021. 
