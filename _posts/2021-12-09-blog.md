---
layout: post
title: How Can We Attack a Contrastive Model?
tags: [contrastive, attack, CLIP]
authors: Paynter, Jonathan, MIT ORC
---

### Goal

This post provides an overview of [*Poisoning and Backdooring Contrastive Learning*](https://arxiv.org/abs/2106.09667) by Carlini and Terzis. I hope that after reading the post you know a bit about contrastive learning, why attacking a constrastive model is different than attacking a supervised ML model, and the key aspects of this paper.

### TL;DR

* A contrastive learning model determines a **representation of unlabeled input data** where elements that are similar are close together, and dissimilar elements elements are **contrasted**, and further apart. 
* Because a contrastive model determines a representation of the data, and does not classify, an **attack on a contrastive model** needs to ensure that a certain input is represented incorrectly - that it is **embedded in the vector space in the 'wrong' area**.
* The primary result for the authors is that by **poisoning just 0.001%** of the input data or applying a patch to 0.005% of the data, their malicious attacks succeed.

### What is contrastive learning?

Carlini and Terzis use CLIP from Open AI ([paper](https://arxiv.org/abs/2103.00020); [blog](https://openai.com/blog/clip/)) as the contrastive model to attack. CLIP was developed to "learn visual concepts from natural language supervision". You can try out an implementation for yourself at [Colab](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb).

A contrastive model learns a representation of the input data, and can then be leveraged in downstream tasks, such as classification or zero-shot learning. For a nice summary of using CLIP for zero-shot learning, see this [blog](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab).  

* **Wait a minute - ** I thought you said this was unlabeled data?

![CLIP-fig1](/_includes/CLIP_fig1.PNG)

[Link](https://github.com/jpaynter/iclr-blog-track.github.io/blob/gh-pages/_includes/CLIP_fig1.PNG)

We can then leverage this new representation of the data in downstream tasks (classification, zero-shot learner, etc).

### A brief description of poisoning and backdooring

* A **poisoning** attack is when an adversary introduces malicious training examples to a machine learning classifier so that it outputs an adversarially-desired label for a certain input. 
* A **backdoor** attack is when an adversary introduces a patch on some training examples so that the machine learning classfier will output an adversarially-desired label anytime the trigger-patch is detected.

### What makes an attack on a contrastive learning model different?

### Poisoning and Backdooring Contrastive Learning

 Carlini and Terzis address both types of attacks on a multi-modal contrastive learner that uses images and captions. They introduce different numbers of malicious samples in different iterations to explore how much of the training data needs to be poisoned or backdoored. 
 
### References 

# This is a template file for a new blog post

$ \sum_{i=0}^j \frac{1}{2^n} \times i $

$$\begin{equation}
a \times b \times c = 0 \\
j=1 \\
k=2 \\
\end{equation}$$

$$\begin{align}
i2 \times b \times c =0 \\
j=1 \\
k=2 \\
\end{align}$$


To bold text, use <strong>.
To italicize text, use <em>.
Abbreviations, like HTML should use <abbr>, with an optional title attribute for the full phrase.
Citations, like â€” Mark otto, should use <cite>.
Deleted text should use <del> and inserted text should use <ins>.
Superscript text uses <sup> and subscript text uses <sub>.
