---
layout: post
title: How Can We Attack a Contrastive Model?
tags: [contrastive, attack, CLIP]
authors: Paynter, Jonathan, MIT ORC
---

### Goal

This post provides an overview of *Poisoning and Backdooring Contrastive Learning* by Carlini and Terzis. I hope that after reading the post you know a bit about contrastive learning, why attacking a constrastive model is different than attacking a supervised ML model, and the key aspects of this paper.

### TL;DR

* A contrastive learning model determines a representation of unlabeled input data where elements that are similar are close together, and dissimilar elements elements are **contrasted**, and further apart. 
* We can then leverage this new representation of the data in downstream tasks.
* A **poisoning** attacker introduces malicious training examples to a machine learning classifier so that it outputs an adversarially-desired label for a certain input. 
* A **backdoor** attacker introduces a patch into the training examples so that the machine learning classfier will output an adversarially-desired label anytime the trigger-patch is detected.
* Because a contrastive model determines a representation of the data, an attack on a contrastive model needs to ensure that a certain input is represented incorrectly - that it is embedded in the vector space in the 'wrong' area.
* Carlini and Terzis approach ...
* Numerical results ...

### What is contrastive learning?
* **Wait a minute - ** I thought you said this was unlabeled data?
### A brief description of poisoning and backdooring

* Outline bullets
* Using a link: [Head to the readme](https://github.com/poole/lanyon#readme)

### What makes an attack on a contrastive learning model different?

### Poisoning and Backdooring Contrastive Learning

### References 

# This is a template file for a new blog post

$ \sum_{i=0}^j \frac{1}{2^n} \times i $

$$\begin{equation}
a \times b \times c = 0 \\
j=1 \\
k=2 \\
\end{equation}$$

$$\begin{align}
i2 \times b \times c =0 \\
j=1 \\
k=2 \\
\end{align}$$


To bold text, use <strong>.
To italicize text, use <em>.
Abbreviations, like HTML should use <abbr>, with an optional title attribute for the full phrase.
Citations, like â€” Mark otto, should use <cite>.
Deleted text should use <del> and inserted text should use <ins>.
Superscript text uses <sup> and subscript text uses <sub>.
